\section[LDA BayesSeg]{Latent Dirichlet Allocation - BayesSeg \label{method: LDA}}
    \begin{table}
    \centering
    \begin{tabular}{lllll}
    \hline
    \textbf{Topic 1}   & \textbf{Topic 2}    & \textbf{Topic 3}   & \textbf{Topic 4} & \textbf{Topic 5}  \\ \hline
    technology   & models        & speakers     & wouldn't   & v\_a\_d     \\
    u\_m\_t\_s   & reverberation & overlaps     & you'd      & worse       \\
    routing      & voicing       & alignment    & agree      & t\_i-digits \\
    transmission & multi-band    & region       & matter     & baseline    \\
    i\_p         & targets       & breath       & depends    & I\_d\_a     \\
    mobile       & phonemes      & laugh        & open       & percent     \\
    packet       & effects       & native       & others     & italian     \\
    university   & echo          & backchannels & feeling    & improvement \\
    concerning   & combining     & laughing     & term       & adaptation  \\
    networking   & insertions    & marks        & opposed    & latency     \\ \hline
    \end{tabular}
    \caption{Sample topics from recorded meeting dialogues, extracted by the modified \gls{lda} algorithm by Purver et al.\cite{purver2006unsupervised}. Words within topics are vaguely related, such as topic 1 concerning networking technology, or topic 3 about conversations. However, a lot of words don't fit well together and are unlikely to represent a topic.}
    \label{table: modified lda topics}
    \end{table}

    One method of segmentation that attempts to extract topic-words is the modified \gls{lda} method \textit{BayesSeg} proposed by Eisenstein et al.\cite{eisenstein2008bayesian} (see Sec. \ref{ssec: topic segmentation}). We choose this method over the similar method presented by Purver et al.\cite{purver2006unsupervised}, because BayesSeg can be applied to individual documents, while the segmentation method by Purver et al. requires a whole corpus of documents. We only received access to the Spotify podcast corpus shortly before the date of submission of this thesis and so could not evaluate said method. However, from the examples shown in the original paper by Purver et al.\cite{purver2006unsupervised}, shown in Table \ref{table: modified lda topics}, we believe the algorithms suffer from similar limitations.

    BayesSeg achieves the WindowDiff penalty score
    \begin{equation}
     w_d = 0.39 \pm 0.06
    \end{equation}
    when segmenting our annotated transcripts. The uncertainty is quite high because we had to annotate evaluation data manually, which is a time-consuming process and was thus limited to 4 transcripts. This is a significantly worse score than the value reported for the ICSI-MRDA corpus, $w_d = 0.312$\cite{eisenstein2008bayesian}.

    We hypothesis that the reduction in performance comes from the change in media: we believe podcast conversations to be more casual, less topic-oriented (less expert knowledge) and less structured than dialogue in meetings, leading to a more difficult segmentation.

    %While the results could agree due to the high uncertainty, we hypothesise the following reason for this reduction in performance instead: \gls{lda} assumes that every word is generated by sampling a topic, which is a distribution over words. Every word is assumed to be part of a topic.
    %In more robust media, such as scientific papers or newspaper articles, this may be an appropriate approximation: if the aim is to convey information efficiently, most words will be related to the topic that is discussed.
    %To an extent, the formal business meetings in the ICSI-MRDA corpus still fit this description, sentences in meetings are trying to convey information efficiently, they largely relate to the topic at hand. In more casual conversations, however, a lot of words are not related to the topic discussed: they act as statements of politeness, as acknowledgements, to fill breaks of awkward silence or to be entertaining\cite{searle1965speech}.
    %This could be an explanation for the worse performance of BayesSeg in casual conversations, and could be further investigated in future work by evaluating performance on more and less formal conversations.

      We identify two further key issues with \gls{lda}-based methods for casual conversation and use examples from the segmentation \gls{model} by Purver et al., shown in Table \ref{table: modified lda topics}, to support them:
    \begin{enumerate}
        \item The lack of correlations captured by \gls{lda} as well as its inability to \gls{model} temporal topic evolution means that unrelated topics can be falsely grouped together. For example ``university" does not fit in Topic 1, and ``Italy" does not fit in Topic 5.
        \item The generative assumption that every word describes a topic leads to topics of words that do not have coherent semantic meaning and appear frequently, reducing the effectiveness of segmentation. An example is Topic 4.
    \end{enumerate}


    %TODO: lda cant do correlations, if two different topics appear together, it sticks them together (t2, t3, t5 (italian)).
    %Also: forcing every word to belong to topic leads to misfits e.g. "combining" in 2, "concerning" in 1,
