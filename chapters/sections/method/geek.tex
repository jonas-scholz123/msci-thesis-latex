\section{GEEK Algorithm}

Utterance embedding based methods (Sec. \ref{method: utterance embedding clustering}) as well as LDA-based methods are somewhat ineffective for the segmentation of conversation in part because they assume that every word in every utterance contributes to the topic of the conversation, which is false.
TopicRank\cite{bougouin-etal-2013-topicrank} (see Sec. \ref{ssec: keyphrase extraction}) offers a promising alternative approach: it first extracts keyphrases that are likely to represent topics. We use this approach and build upon it to develop an algorithm that both segments and labels topics. We call this improved algorithm the \textbf{G}raph of \textbf{E}mbedded \textbf{E}xtracted \textbf{K}eyphrases (GEEK) algorithm. GEEK extracts keyphrases, but uses embeddings to achieve a better selection than TopicRank. It then uses embedding-based similarity to determine cohesive sections of the transcript.

GEEK is based on the following assumptions:

\begin{enumerate}
    \item Topics are described by a small number of keywords. We assume keywords to exclusively be nouns, proper nouns and named entities, such as \textit{family}, \textit{music}, \textit{Donald Trump, the CIA} or \textit{Tesla}.
    \item Topics are sections within the text that feature similar keywords. Keywords that are only mentioned once can not represent a topic.
    \item There can be more than one active topic: topics can overlap and smaller sub-topics can be discussed within larger topics.
    \item If the gap between related keywords is too large, they do not belong to the same segment.
\end{enumerate}

    \subsection{Hyperparameters}
    The hyperparameters that the user needs to set to use GEEK are as follows:
    
    \newcolumntype{t}{>{\hsize=.15\hsize}X}
    \newcolumntype{b}{>{\hsize=.7\hsize}X}
    \begin{table}[h]
        \centering
        \begin{tabularx}{0.9\textwidth}{| t | b | t |}
        \hline
        \textbf{Symbol} & \textbf{Description} & \textbf{Value} \\ \hline
        $\Delta_{\text{max}}$     & The maximum gap between matching key-words for them to be considered in the same segment & 11                    \\ \hline
        $\eta_{\text{min}}$     & The minimum cosine similarity of two word embeddings for their words to be considered similar. & 0.55                        \\ \hline
        $T_{\text{min}}$ & Minimum number of utterances a topic must span. & 3 \\ \hline
        
        \end{tabularx}
    \end{table}

    \subsection{Keyword Extraction}
        TopicRank extracts noun phrases such as
        \begin{equation*}
            ``\textbf{The spotted puppy}\text{ is up for adoption.}",
        \end{equation*}
        as keyphrases. We instead only extract the nouns, proper nouns and named entities themselves (and no surrounding adjectives/articles). We do this because most multi-word noun phrases do not have pre-trained embeddings. However, if a two-word phrase ends in a noun and is a part of the embeddings it is also added as a keyword to include phrases such as ``neural net" and ``absolute zero". The embeddings used here are the \textit{ConceptNet Numberbatch mini} embeddings (see Sec.\ref{ssec: Numberbatch}).
        
        \subsubsection{POS Tagging}
            We extract POS tags using the pre-trained POS-tagger by the python library Flair\cite{flairNLP}, which achieves state of the art results. It tags every word with its grammatical function and allows us to extract nouns and proper nouns (see Sec. \ref{sssec: POS tagging}).
        
        \subsubsection{Named Entity Recognition}
            We use the pre-trained named entity recognition (NER) tagger from the python library Flair\cite{flairNLP}, which also achieves state of the art results. This allows us to extract the names of persons, companies, dates, products etc. as keywords.
        
        \subsubsection{Post Processing}
            Some words are technically nouns but are unwanted as keywords. They include words such as ``dude", ``everything" or ``dope". We manually exclude 89 such words from keywords.
        
    \subsection{Determining Keyword Ranges}
        After we extract keywords, we determine the range of utterances over which it is ``active". We do this by starting at the utterance containing a keyword $u_i$ and searching the next $\Delta_{\text{max}}$ utterances for matches, where matches are defined as keywords whose embeddings $\mathbf{w_i}, \mathbf{w_j}$ have a similarity
        \begin{equation}
            \text{cosine}(\mathbf{w_i}, \mathbf{w_j}) \geq \eta_{\text{min}}.
        \end{equation}
        If a match is found at utterance $u_j$, the search is repeated with the \textbf{same keyword}, this time starting from $u_j$. This is repeated until no match is found. Then the end of the range is defined to be the last occurrence of a match. If the range is smaller than $T_{\text{min}}$, the range is discarded. 
        
    \subsection{Joining Topic Ranges}
        Now the Keyword ranges are joined so that similar overlapping keywords are joined. To do this, all keyword ranges $k_i$ are represented as nodes in a Graph. Two nodes $k_i$ and $k_j$ are connected if $k_i$ and $k_j$ match and if the ranges of $k_i$ and $k_j$ overlap. We essentially follow the minimum cut procedure discussed in Sec. \ref{method: utterance embedding clustering}, but instead of sentences, we cluster keywords. The clustered $k_i$ represent the final topic labels, their ranges determine the final segmentation. An example of the final outcome that shows many of GEEK's features is visualised in Fig. \ref{fig:GEEK final result}.
        
        \begin{figure}
            \centering
            \includegraphics[width=\textwidth]{joe_rogan_jack_dorsey290_370_topics.pdf}
            \caption{An excerpt of a conversation between podcast host Joe Rogan and twitter CEO Jack Dorsey. The y-axis is used to display simultaneous topics. Similar keywords are grouped together into topics. Topics are colour-coded by mapping embeddings to a colour.}
            \label{fig:GEEK final result}
        \end{figure}
    
    \subsection{Evaluation}
    
    \subsubsection{Segmentation}
        Even though segmentation is not exactly possible as there are multiple topics that may overlap, by simply placing a topic boundary at the start of the largest topic out of a group of overlapping topics, we can achieve a reasonable approximation. This is visualised in Fig. \ref{fig:GEEK segment}.
        \begin{figure}[h]
             \centering
             \begin{subfigure}[b]{\textwidth}
                 \centering
                 \includegraphics[width=1\textwidth]{figures/0to100segment_topics.pdf}
                 \caption{$u_0$ to $u_100$}
             \end{subfigure}
             \hfill
             \begin{subfigure}[b]{\textwidth}
                 \centering
                 \includegraphics[width=1\textwidth]{figures/100to200segment_topics.pdf}
             \end{subfigure}
                \caption{200 utterances out of the evaluation transcript of a conversation between host Joe Rogan and guest Elon Musk. Vertical lines indicate boundaries: Green lines indicate human boundaries, red lines indicate boundaries placed by GEEK and black lines are boundaries where the two agree exactly. Most boundaries are exactly or approximately detected by GEEK. Failures include $u_4$ (missed topic change) or $u_{110}$ (false boundary).}
                \label{fig:GEEK segment}
        \end{figure}
        
        Using this approximation, GEEK achieves a state of the art windowDiff score of
        \begin{equation}
            w_d = 0.217 \pm 0.44.
        \end{equation}
    
    
    \subsubsection{Labelling}
        %TODO: reword
        To evaluate GEEK's labels, we hand-label the annotated evaluation transcripts and check what fraction of the concurrent GEEK topics within any segment match with the human labels, defined as having a cosine similarity of 0.55 or greater. This is not a standard metric, but usually labels are not evaluated.
    
    \subsection{Limitations}
    GEEK still suffers from some limitations that are likely difficult to solve.
    
    \subsubsection{False Keywords}
        Some words are nouns and are used in conversation but do not describe topics. While GEEK is somewhat robust against this, by requiring keywords to appear at least twice at least $T_{\text{min}}$ utterances apart, sometimes it fails. An example is the phrase ``attack vector" that Elon Musk uses in the conversation shown in Fig. \ref{fig:GEEK segment}. This phrase is a non-standard way of saying ``point of weakness", but because Elon Musk, and eventually the host Joe Rogan, keep using it, it is detected by GEEK as two keywords ``attack" and ``vector". This leads to a wrong segmentation (the red line at $u_{110})$.
    
    \subsubsection{Missed Topics}
        In some cases GEEK also suffers from the opposite issue: minimum topic length may discard some actual topics that happen to be over quickly. An example for that is a missed boundary at $u_4$ in Fig. \ref{fig:GEEK segment}.
        
    \subsubsection{Ambiguous Embeddings}
        Certain matches are not detected because words can not be disambiguated (see Sec. \ref{fig:ambiguous_embedding}). For example, in Fig. \ref{fig:GEEK final result}, the keyword ``post" should be included in the topic ``hasthag, instagram twitter, tweets, facebook", because ``post" and ``tweets" should be considered similar. Unfortunately, the word ``post" has ambiguous meanings, which leads to a poor embedding. To fix this, newer transformer embeddings such as ElMo\cite{peters2018elmo} or BERT\cite{devlin2018bert}, that make embeddings context-dependent and thus disambiguated could be evaluated in future work.
        
