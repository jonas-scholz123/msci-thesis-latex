
\section{Dialogue Act Classification \label{ssec: da classification}}
    %\Glspl{da} (see Sec. \ref{ssec: DAs}) are often applied within the context of adjacency pairs (see \ref{ssec: adjacency pairs}). They have been used to analyse performance appraisal interviews\cite{ap_interview}, predict psychological disorders such as depression or social anxiety\cite{ap_psychological} or analyse politician's speech patterns\cite{ap_trump}.
    %A lot of these are based on manually annotated transcriptions or are purely qualitative in nature. To automate \gls{da} annotation would make it significantly easier for researchers to process large data sets and quantify their findings.
    All methods of classifying \glspl{da} rely on \gls{ml} \glspl{model}. Such \glspl{model}, $\hat{f}_{da}$ could be written as
    \begin{equation}
        \hat{f}_{da}: u_i \rightarrow d_i,
    \end{equation}
    where an \gls{utterance} $u_i$ is mapped to a \gls{da} label $d_i$. Better \glspl{model}, however, don't just map one \gls{utterance} to one label, but consider the whole sequence of \glspl{utterance} $U$ i.e.
    \begin{equation}
        \hat{f}_{da}: U \rightarrow D, \hspace{3em} u_i \in U, \hspace{0.5em} d_i \in D \hspace{0.5em}.
    \end{equation}
    To do this, all state of the art \glspl{model} use \glspl{rnn} (see Sec. \ref{ssec: RNNs}) to make their classifications. \Gls{da} classification is an active area of research and different combinations of \glspl{rnn} are proposed to improve \gls{model} performance. State of the art \glspl{model} for \gls{da} classification are summarised by Ruder, 2021 \cite{DAgithub}. The comparison of state of the art \glspl{model} is shown in table \ref{table: da models}.

    \begin{table}[ht]
    \centering
    \begin{tabular}{|l|l|l|}
    \hline
    \textbf{Model}                   & \textbf{Accuracy} & \textbf{Paper / Source}      \\ \hline
    SGNN                             & 83.1              & Ravi et al., 2018 \cite{ravi2018self}   \\ \hline
    CASA                             & 82.9              & Raheja et al., 2019\cite{raheja2019dialogue} \\ \hline
    DAH-CRF                          & 82.3              & Li et al., 2019 \cite{li2018dual}     \\ \hline
    ALDMN                            & 81.5              & Wan et al., 2018 \cite{wan2018improved}    \\ \hline
    CRF-ASN                          & 81.3              & Chen et al., 2018 \cite{chen2018dialogue}   \\ \hline
    Bi-LSTM-CRF                      & 79.2              & Kumar et al., 2017 \cite{kumar2017dialogue}  \\ \hline
    RNN with 3 utterances in context & 77.34             & Bothe et al., 2018 \cite{bothe2018context}  \\ \hline
    \end{tabular}
    \caption{State of the art \glspl{model} and their accuracies. Trained and evaluated on the \gls{swda} corpus. There was only 84\% agreement among human annotators of the \gls{swda} corpus, so the best \glspl{model} are almost as accurate as humans.\cite{swda}.}
    \label{table: da models}
    \end{table}

    \subsection{Switchboard Dialogue Act Corpus \label{ssec: swda}}
        The \gls{swda} corpus is a collection of 1155 five-minute conversations between two participants, in which callers question receivers on pre-determined topics such as child care, recycling and new media.\cite{fang2012annotation}. It is transcribed by humans and annotated with \glspl{da} (see Sec. \ref{ssec: DAs}). It is commonly used as training data for classifiers and to evaluate said classifiers.

    \subsection{Bi-LSTM-CRF \label{sssec: kumar model}}
    For our work, we select the Bi-LSTM-CRF \gls{model}\cite{kumar2017dialogue}, because of relatively high \gls{model} accuracy, relative \gls{model} simplicity, and because the authors released their source code. 
    %It combines a bidirectional \glspl{rnn} using \gls{lstm} \glspl{neuron} (see Sec. \ref{ssec: RNNs}) with a \gls{crf} (see Sec. \ref{fig: HMM and CRF}) layer and word \glspl{embedding}. 
    Since we made changes to the \gls{model} and re-implemented it, the explanation of how the Bi-LSTM-CRF \gls{model} works along with a plot of the structure of the \gls{model} (Fig. \ref{fig:kumar_model}) can be found in Sec. \ref{method: kumar model}.
