\chapter{Conclusions}
We aimed to evaluate a wide range of natural language processing techniques for dialogue act and topic analysis within the context of conversations and to improve them and we succeeded in doing so.
\subsubsection{State of the Art Dialogue Act Classification}
    For dialogue act classification, we select a 2017 model by Kumar et al., re-implement it within a newer python framework, fix an important bug and modernise components such as the word embeddings and recurrent neural network architecture, improving it from an accuracy of $79.2 \pm 0.3\%$ in the \gls{swda} Corpus to an accuracy of $84.6 \pm 0.3\%$, beating the previous state of the art accuracy of $83.1$ as well as the inter-annotator agreement accuracy of $84\%$ between the annotators who created the corpus.

\subsubsection{State of the Art Conversation Topic Modelling}
    Within the area of topic modelling, specifically when applying it to conversations, research is much less active than within dialogue act classification, and no standard corpora exist for evaluation. Therefore, we hand-annotated 4 podcast transcript excerpts and evaluated models against those. We evaluated it and other methods, such as utterance embedding based clustering, via a minimum-cut graph following the work of Malioutov et al., and other unsupervised clustering methods, but qualitatively achieve such underwhelming results that we do not attempt a quantitative evaluation. The previous state of the art topic-model, BayesSeg, performs better, but still does not achieve satisfactory performance, a \textit{windowDiff} penalty score of $0.39 \pm 0.06$ for topic segmentation.
    By identifying the key problem in all these approaches, the fact that many words in conversations are unrelated to the topic and therefore dilute seperation of dissimilar topics, we develop our own method: a graph of embedded extracted keyphrases (GEEK).
    GEEK follows a modified version of the TopicRank approach by Bougouin et al. to extract key-words and multi-word phrases that are likely to represent topics and then uses word embeddings of these key-words to identify semantically cohesive sections as topics with their keywords as topic labels. We find that GEEK is significantly more robust against weak key-word candidates than TopicRank by using semantically similar words to determine importance of candidates, not just repetitions of the exact candidate, and by filtering a list of 89 manually determined ``false flag" key-word candidates. We also improve the overall selection of keyphrases over TopicRank by employing named entity recognition to extract named entities (such as ``Donald Trump" or ``CIA"), which often indicate topics. Quantitatively, GEEK significantly improves upon the state of the art topic segmentation by BayesSeg, achieving the much lower \textit{windowDiff} penalty score of $\mathbf{0.22 \pm 0.04}$. We emphasize that this score is likely somewhat biased because we both \textit{created} the segmentation algorithm and annotated the dataset with which it is \textit{evaluated}, both of which are subject to our own biases as to what constitutes a topic and what constitutes a change of topic. However, we are confident that GEEK would still outperform the previous state of the art in an independently annotated dataset and propose the creation of such a dataset as a standardised metric to encourage and legitimise future work in this area.

\subsubsection{Creation of a Public Corpus of Annotated Podcast Transcripts}
    We parallelise and optimise our analysis methods to annotate a large selection of 18000 podcasts from the Spotify podcast corpus\cite{clifton-2020100000} with dialogue acts, keywords and topics. This corpus can be used to study a wide range of linguistic questions, such as \dots. Upon request, we make it available to any researchers that have also been granted access to the Spotify podcast corpus.
\glsresetall

\newpage